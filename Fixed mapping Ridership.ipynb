{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9760f4-fc64-467c-aa40-c5450a96a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed category lists saved to fixed_categories.py\n"
     ]
    }
   ],
   "source": [
    "def unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in seq:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in seq:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "# --- Load your data used for training the model ---\n",
    "ridership_df = pd.read_csv('/Users/fahmi.taib/Desktop/Deployment Code Test/very_new_finalised_ridership_route.csv', low_memory=False)\n",
    "ridership_df['date'] = pd.to_datetime(ridership_df['date'], format='%m/%d/%y', errors='coerce')\n",
    "ridership_df = ridership_df.dropna(subset=['date'])\n",
    "\n",
    "ranked_routes = pd.read_csv('/Users/fahmi.taib/Desktop/Deployment Code Test/top_200_routes_by_ridership.csv')\n",
    "top_routes = ranked_routes.head(180)['route_no'].astype(str)\n",
    "ridership_df = ridership_df[ridership_df['route_no'].astype(str).isin(top_routes)]\n",
    "\n",
    "agg_df = ridership_df.groupby(['route_no', 'date']).agg({\n",
    "    'ridership_total': 'sum',\n",
    "    'hour': 'max',\n",
    "    'depot': 'first',\n",
    "    'route_id': 'first'  # Keep route_id here to get its categories, even if not used in model features\n",
    "}).reset_index()\n",
    "\n",
    "depot_categories = [str(x) for x in [\"29\", \"7\", \"10\", \"22\", \"27\", \"2\", \"5\", \"37\", \"4\", \"38\"]]\n",
    "\n",
    "route_no_categories = [str(x) for x in [\n",
    "    \"PJ01\", \"100\", \"200\", \"201\", \"300\", \"302\", \"303\", \"400\", \"401\", \"500\", \"600\", \"601\", \"T504\", \"T505\",\n",
    "    \"T506\", \"T507\", \"T508\", \"T509\", \"T510\", \"T511\", \"T512\", \"T545\", \"301\", \"T201\", \"SJ01\", \"SA03\", \"SEWA1\",\n",
    "    \"T715\", \"T753\", \"T754\", \"T756\", \"T774\", \"T776\", \"T778\", \"T780\", \"T781\", \"T782\", \"T786\", \"T787\", \"T788\",\n",
    "    \"T789\", \"T790\", \"T791\", \"402\", \"751\", \"752\", \"754\", \"770\", \"771\", \"783\", \"SA08\", \"Subang HQ\", \"T783\",\n",
    "    \"T850\", \"708\", \"750\", \"753\", \"782\", \"T562\", \"T785\", \"SA01\", \"SA02\", \"SU13A\", \"T300\", \"T301\", \"T302\",\n",
    "    \"MAHA1\", \"T304\", \"T406\", \"T571\", \"T406B\", \"T602\", \"506\", \"540\", \"T250\", \"T450\", \"T580\", \"T604\", \"202\",\n",
    "    \"250\", \"251\", \"253\", \"590\", \"602\", \"641\", \"650\", \"651\", \"652\", \"T581\", \"T601\", \"T605\", \"T640\", \"580\",\n",
    "    \"581\", \"541\", \"T221\", \"151\", \"173\", \"180\", \"191\", \"220\", \"222\", \"780\", \"801\", \"802\", \"T202\", \"T203\",\n",
    "    \"T203B\", \"T222\", \"254\", \"772\", \"821\", \"T251\", \"BET16\", \"PBD1\", \"T200\", \"190\", \"SEWA 2\", \"T603\", \"170\",\n",
    "    \"822\", \"851\", \"T600\", \"171\", \"T582\", \"640\", \"T221B\", \"BET17\", \"P101\", \"P102\", \"P103\", \"P105\", \"P106\",\n",
    "    \"P108\", \"T407\", \"T569\", \"T567\", \"T568\", \"MS01\", \"DS01\", \"DS01(PM)\", \"420\", \"450\", \"PAVILION BUKI\",\n",
    "    \"AJ2A\", \"T783B\", \"AJ03\", \"T785B\", \"SEWA3\", \"PTPM\", \"GP03\", \"T566\", \"T559\", \"T543\", \"T582B\", \"T757B\",\n",
    "    \"T774B\", \"T786B\", \"KJ03\", \"T542\", \"T757\", \"T778B\", \"421\", \"T224\", \"T223\", \"T350\", \"KLG2A\", \"T303\",\n",
    "    \"T351\", \"SA04\", \"T120\", \"HLB1\", \"451\", \"SA06\", \"T173\"\n",
    "]]\n",
    "\n",
    "route_id_categories = [str(x) for x in [\n",
    "    \"1219\", \"736\", \"215\", \"218\", \"219\", \"220\", \"222\", \"223\", \"224\", \"225\", \"663\", \"226\", \"227\", \"228\", \"1081\",\n",
    "    \"1082\", \"1083\", \"1084\", \"1085\", \"1086\", \"1087\", \"1088\", \"1089\", \"1179\", \"221\", \"853\", \"531\", \"667\", \"1152\",\n",
    "    \"566\", \"570\", \"571\", \"573\", \"651\", \"653\", \"655\", \"724\", \"725\", \"723\", \"730\", \"731\", \"732\", \"733\", \"729\",\n",
    "    \"734\", \"497\", \"498\", \"500\", \"501\", \"502\", \"721\", \"940\", \"741\", \"726\", \"742\", \"495\", \"496\", \"499\", \"720\",\n",
    "    \"1096\", \"728\", \"530\", \"666\", \"993\", \"1248\", \"1247\", \"1245\", \"1243\", \"1249\", \"1228\", \"1227\", \"1246\", \"1212\",\n",
    "    \"1221\", \"548\", \"1013\", \"1222\", \"1223\", \"631\", \"1242\", \"480\", \"546\", \"1241\", \"547\", \"1274\", \"633\", \"1277\",\n",
    "    \"956\", \"1279\", \"558\", \"1275\", \"616\", \"1276\", \"594\", \"858\", \"559\", \"673\", \"629\", \"877\", \"848\", \"849\", \"850\",\n",
    "    \"478\", \"488\", \"679\", \"716\", \"717\", \"671\", \"672\", \"674\", \"615\", \"630\", \"718\", \"632\", \"486\", \"487\", \"613\",\n",
    "    \"856\", \"1110\", \"873\", \"874\", \"876\", \"844\", \"846\", \"847\", \"835\", \"837\", \"838\", \"854\", \"855\", \"1213\", \"857\",\n",
    "    \"851\", \"834\", \"880\", \"859\", \"860\", \"904\", \"852\", \"875\", \"1155\", \"845\", \"647\", \"678\", \"870\", \"878\", \"881\",\n",
    "    \"882\", \"646\", \"871\", \"675\", \"714\", \"1206\", \"861\", \"1060\", \"1061\", \"1062\", \"1064\", \"1065\", \"1066\", \"755\",\n",
    "    \"1182\", \"1101\", \"1102\", \"1161\", \"1002\", \"1004\", \"474\", \"481\", \"483\", \"990\", \"477\", \"790\", \"1208\", \"173\",\n",
    "    \"792\", \"1210\", \"1250\", \"1178\", \"1187\", \"1100\", \"1030\", \"1091\", \"1207\", \"1126\", \"1214\", \"1209\", \"797\",\n",
    "    \"1090\", \"656\", \"1205\", \"482\", \"866\", \"865\", \"549\", \"779\", \"606\", \"550\", \"668\", \"884\", \"1015\", \"1159\", \"923\"\n",
    "]]\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "depot_categories = unique_preserve_order(depot_categories)\n",
    "route_no_categories = unique_preserve_order(route_no_categories)\n",
    "route_id_categories = unique_preserve_order(route_id_categories)\n",
    "\n",
    "# Save to Python file for import\n",
    "with open('/Users/fahmi.taib/Desktop/Deployment Code Test/fixed_categories.py', 'w') as f:\n",
    "    f.write(\"# Auto-generated fixed category lists for consistent encoding\\n\\n\")\n",
    "    f.write(f\"depot_categories = {json.dumps(depot_categories, indent=4)}\\n\\n\")\n",
    "    f.write(f\"route_no_categories = {json.dumps(route_no_categories, indent=4)}\\n\\n\")\n",
    "    f.write(f\"route_id_categories = {json.dumps(route_id_categories, indent=4)}\\n\")\n",
    "\n",
    "print(\"Fixed category lists saved to fixed_categories.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a98056-ba3e-42fd-acda-df785858319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 176\n",
      "Duplicates before cleaning: ['402']\n",
      "Length after cleaning: 175\n",
      "Duplicates after cleaning: []\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "def find_duplicates(seq):\n",
    "    counts = Counter(seq)\n",
    "    return [item for item, count in counts.items() if count > 1]\n",
    "\n",
    "def unique_preserve_order_clean(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in seq:\n",
    "        item_clean = str(item).strip().lower()\n",
    "        if item_clean not in seen:\n",
    "            seen.add(item_clean)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "route_no_categories = [str(x) for x in [\n",
    "    \"402\",\"PJ01\", \"100\", \"200\", \"201\", \"300\", \"302\", \"303\", \"400\", \"401\", \"500\", \"600\", \"601\", \"T504\", \"T505\",\n",
    "    \"T506\", \"T507\", \"T508\", \"T509\", \"T510\", \"T511\", \"T512\", \"T545\", \"301\", \"T201\", \"SJ01\", \"SA03\", \"SEWA1\",\n",
    "    \"T715\", \"T753\", \"T754\", \"T756\", \"T774\", \"T776\", \"T778\", \"T780\", \"T781\", \"T782\", \"T786\", \"T787\", \"T788\",\n",
    "    \"T789\", \"T790\", \"T791\", \"402\", \"751\", \"752\", \"754\", \"770\", \"771\", \"783\", \"SA08\", \"Subang HQ\", \"T783\",\n",
    "    \"T850\", \"708\", \"750\", \"753\", \"782\", \"T562\", \"T785\", \"SA01\", \"SA02\", \"SU13A\", \"T300\", \"T301\", \"T302\",\n",
    "    \"MAHA1\", \"T304\", \"T406\", \"T571\", \"T406B\", \"T602\", \"506\", \"540\", \"T250\", \"T450\", \"T580\", \"T604\", \"202\",\n",
    "    \"250\", \"251\", \"253\", \"590\", \"602\", \"641\", \"650\", \"651\", \"652\", \"T581\", \"T601\", \"T605\", \"T640\", \"580\",\n",
    "    \"581\", \"541\", \"T221\", \"151\", \"173\", \"180\", \"191\", \"220\", \"222\", \"780\", \"801\", \"802\", \"T202\", \"T203\",\n",
    "    \"T203B\", \"T222\", \"254\", \"772\", \"821\", \"T251\", \"BET16\", \"PBD1\", \"T200\", \"190\", \"SEWA 2\", \"T603\", \"170\",\n",
    "    \"822\", \"851\", \"T600\", \"171\", \"T582\", \"640\", \"T221B\", \"BET17\", \"P101\", \"P102\", \"P103\", \"P105\", \"P106\",\n",
    "    \"P108\", \"T407\", \"T569\", \"T567\", \"T568\", \"MS01\", \"DS01\", \"DS01(PM)\", \"420\", \"450\", \"PAVILION BUKI\",\n",
    "    \"AJ2A\", \"T783B\", \"AJ03\", \"T785B\", \"SEWA3\", \"PTPM\", \"GP03\", \"T566\", \"T559\", \"T543\", \"T582B\", \"T757B\",\n",
    "    \"T774B\", \"T786B\", \"KJ03\", \"T542\", \"T757\", \"T778B\", \"421\", \"T224\", \"T223\", \"T350\", \"KLG2A\", \"T303\",\n",
    "    \"T351\", \"SA04\", \"T120\", \"HLB1\", \"451\", \"SA06\", \"T173\"\n",
    "]]\n",
    "\n",
    "print(f\"Original length: {len(route_no_categories)}\")\n",
    "print(\"Duplicates before cleaning:\", find_duplicates(route_no_categories))\n",
    "\n",
    "# Remove None/NaN\n",
    "route_no_categories = [x for x in route_no_categories if pd.notna(x)]\n",
    "\n",
    "# Deduplicate with cleaning\n",
    "route_no_categories = unique_preserve_order_clean(route_no_categories)\n",
    "\n",
    "print(f\"Length after cleaning: {len(route_no_categories)}\")\n",
    "print(\"Duplicates after cleaning:\", find_duplicates(route_no_categories))\n",
    "\n",
    "# Now safe to create CategoricalDtype\n",
    "route_no_cat_type = CategoricalDtype(categories=route_no_categories, ordered=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbfd0acc-fbd7-47e8-b775-fb8a7ea6766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ridership with estimates saved successfully.\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters found: {'colsample_bytree': np.float64(0.6174415087017735), 'gamma': np.float64(4.972752553986705), 'learning_rate': np.float64(0.1509833541972829), 'max_depth': 8, 'n_estimators': 472, 'reg_alpha': np.float64(0.883494022266259), 'reg_lambda': np.float64(0.7477187738974139), 'subsample': np.float64(0.9812287388095813)}\n",
      "\n",
      "XGBoost Model Evaluation:\n",
      "MAE: 123.82\n",
      "MSE: 288485.98\n",
      "RMSE: 537.11\n",
      "R-squared: 0.9093\n",
      "Model and scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint, uniform\n",
    "import joblib\n",
    "\n",
    "# --- Manually define fixed mappings as dictionaries ---\n",
    "depot_mapping = {\n",
    "    \"29\": 0,\n",
    "    \"7\": 1,\n",
    "    \"10\": 2,\n",
    "    \"22\": 3,\n",
    "    \"27\": 4,\n",
    "    \"2\": 5,\n",
    "    \"5\": 6,\n",
    "    \"37\": 7,\n",
    "    \"4\": 8,\n",
    "    \"38\": 9,\n",
    "}\n",
    "\n",
    "route_no_mapping = {\n",
    "    \"PJ01\": 0,\n",
    "    \"100\": 1,\n",
    "    \"200\": 2,\n",
    "    \"201\": 3,\n",
    "    \"300\": 4,\n",
    "    \"302\": 5,\n",
    "    \"303\": 6,\n",
    "    \"400\": 7,\n",
    "    \"401\": 8,\n",
    "    \"500\": 9,\n",
    "    \"600\": 10,\n",
    "    \"601\": 11,\n",
    "    \"T504\": 12,\n",
    "    \"T505\": 13,\n",
    "    \"T506\": 14,\n",
    "    \"T507\": 15,\n",
    "    \"T508\": 16,\n",
    "    \"T509\": 17,\n",
    "    \"T510\": 18,\n",
    "    \"T511\": 19,\n",
    "    \"T512\": 20,\n",
    "    \"T545\": 21,\n",
    "    \"301\": 22,\n",
    "    \"T201\": 23,\n",
    "    \"SJ01\": 24,\n",
    "    \"SA03\": 25,\n",
    "    \"SEWA1\": 26,\n",
    "    \"T715\": 27,\n",
    "    \"T753\": 28,\n",
    "    \"T754\": 29,\n",
    "    \"T756\": 30,\n",
    "    \"T774\": 31,\n",
    "    \"T776\": 32,\n",
    "    \"T778\": 33,\n",
    "    \"T780\": 34,\n",
    "    \"T781\": 35,\n",
    "    \"T782\": 36,\n",
    "    \"T786\": 37,\n",
    "    \"T787\": 38,\n",
    "    \"T788\": 39,\n",
    "    \"T789\": 40,\n",
    "    \"T790\": 41,\n",
    "    \"T791\": 42,\n",
    "    \"402\": 43,\n",
    "    \"751\": 44,\n",
    "    \"752\": 45,\n",
    "    \"754\": 46,\n",
    "    \"770\": 47,\n",
    "    \"771\": 48,\n",
    "    \"783\": 49,\n",
    "    \"SA08\": 50,\n",
    "    \"Subang HQ\": 51,\n",
    "    \"T783\": 52,\n",
    "    \"T850\": 53,\n",
    "    \"708\": 54,\n",
    "    \"750\": 55,\n",
    "    \"753\": 56,\n",
    "    \"782\": 57,\n",
    "    \"T562\": 58,\n",
    "    \"T785\": 59,\n",
    "    \"SA01\": 60,\n",
    "    \"SA02\": 61,\n",
    "    \"SU13A\": 62,\n",
    "    \"T300\": 63,\n",
    "    \"T301\": 64,\n",
    "    \"T302\": 65,\n",
    "    \"MAHA1\": 66,\n",
    "    \"T304\": 67,\n",
    "    \"T406\": 68,\n",
    "    \"T571\": 69,\n",
    "    \"T406B\": 70,\n",
    "    \"T602\": 71,\n",
    "    \"506\": 72,\n",
    "    \"540\": 73,\n",
    "    \"T250\": 74,\n",
    "    \"T450\": 75,\n",
    "    \"T580\": 76,\n",
    "    \"T604\": 77,\n",
    "    \"202\": 78,\n",
    "    \"250\": 79,\n",
    "    \"251\": 80,\n",
    "    \"253\": 81,\n",
    "    \"590\": 82,\n",
    "    \"602\": 83,\n",
    "    \"641\": 84,\n",
    "    \"650\": 85,\n",
    "    \"651\": 86,\n",
    "    \"652\": 87,\n",
    "    \"T581\": 88,\n",
    "    \"T601\": 89,\n",
    "    \"T605\": 90,\n",
    "    \"T640\": 91,\n",
    "    \"580\": 92,\n",
    "    \"581\": 93,\n",
    "    \"541\": 94,\n",
    "    \"T221\": 95,\n",
    "    \"151\": 96,\n",
    "    \"173\": 97,\n",
    "    \"180\": 98,\n",
    "    \"191\": 99,\n",
    "    \"220\": 100,\n",
    "    \"222\": 101,\n",
    "    \"780\": 102,\n",
    "    \"801\": 103,\n",
    "    \"802\": 104,\n",
    "    \"T202\": 105,\n",
    "    \"T203\": 106,\n",
    "    \"T203B\": 107,\n",
    "    \"T222\": 108,\n",
    "    \"254\": 109,\n",
    "    \"772\": 110,\n",
    "    \"821\": 111,\n",
    "    \"T251\": 112,\n",
    "    \"BET16\": 113,\n",
    "    \"PBD1\": 114,\n",
    "    \"T200\": 115,\n",
    "    \"190\": 116,\n",
    "    \"SEWA 2\": 117,\n",
    "    \"T603\": 118,\n",
    "    \"170\": 119,\n",
    "    \"822\": 120,\n",
    "    \"851\": 121,\n",
    "    \"T600\": 122,\n",
    "    \"171\": 123,\n",
    "    \"T582\": 124,\n",
    "    \"640\": 125,\n",
    "    \"T221B\": 126,\n",
    "    \"BET17\": 127,\n",
    "    \"P101\": 128,\n",
    "    \"P102\": 129,\n",
    "    \"P103\": 130,\n",
    "    \"P105\": 131,\n",
    "    \"P106\": 132,\n",
    "    \"P108\": 133,\n",
    "    \"T407\": 134,\n",
    "    \"T569\": 135,\n",
    "    \"T567\": 136,\n",
    "    \"T568\": 137,\n",
    "    \"MS01\": 138,\n",
    "    \"DS01\": 139,\n",
    "    \"DS01(PM)\": 140,\n",
    "    \"420\": 141,\n",
    "    \"450\": 142,\n",
    "    \"PAVILION BUKIT JALIL\": 143,\n",
    "    \"AJ2A\": 144,\n",
    "    \"T783B\": 145,\n",
    "    \"AJ03\": 146,\n",
    "    \"T785B\": 147,\n",
    "    \"SEWA3\": 148,\n",
    "    \"PTPM\": 149,\n",
    "    \"GP03\": 150,\n",
    "    \"T566\": 151,\n",
    "    \"T559\": 152,\n",
    "    \"T543\": 153,\n",
    "    \"T582B\": 154,\n",
    "    \"T757B\": 155,\n",
    "    \"T774B\": 156,\n",
    "    \"T786B\": 157,\n",
    "    \"KJ03\": 158,\n",
    "    \"T542\": 159,\n",
    "    \"T757\": 160,\n",
    "    \"T778B\": 161,\n",
    "    \"421\": 162,\n",
    "    \"T224\": 163,\n",
    "    \"T223\": 164,\n",
    "    \"T350\": 165,\n",
    "    \"KLG2A\": 166,\n",
    "    \"T303\": 167,\n",
    "    \"T351\": 168,\n",
    "    \"SA04\": 169,\n",
    "    \"T120\": 170,\n",
    "    \"HLB1\": 171,\n",
    "    \"451\": 172,\n",
    "    \"SA06\": 173,\n",
    "    \"T173\": 174\n",
    "}\n",
    "\n",
    "# --- Load ranked routes ---\n",
    "ranked_routes = pd.read_csv('/Users/fahmi.taib/Desktop/Deployment Code Test/top_200_routes_by_ridership.csv')\n",
    "\n",
    "# --- Load ridership data ---\n",
    "ridership_df = pd.read_csv('/Users/fahmi.taib/Desktop/Deployment Code Test/very_new_finalised_ridership_route.csv', low_memory=False)\n",
    "ridership_df['date'] = pd.to_datetime(ridership_df['date'], format='%m/%d/%y', errors='coerce')\n",
    "ridership_df = ridership_df.dropna(subset=['date'])\n",
    "\n",
    "# --- Load holiday data ---\n",
    "holiday_df = pd.read_csv('/Users/fahmi.taib/Desktop/Deployment Code Test/Holiday 2024.csv')\n",
    "holiday_df['StartDate'] = holiday_df['StartDate'].astype(str) + '-2024'\n",
    "holiday_df['EndDate'] = holiday_df['EndDate'].astype(str) + '-2024'\n",
    "holiday_df['StartDate'] = pd.to_datetime(holiday_df['StartDate'], format='%d-%b-%Y', errors='coerce')\n",
    "holiday_df['EndDate'] = pd.to_datetime(holiday_df['EndDate'], format='%d-%b-%Y', errors='coerce')\n",
    "\n",
    "all_holiday_dates = []\n",
    "for _, row in holiday_df.iterrows():\n",
    "    if pd.isna(row['StartDate']) or pd.isna(row['EndDate']):\n",
    "        continue\n",
    "    all_holiday_dates.extend(pd.date_range(start=row['StartDate'], end=row['EndDate']))\n",
    "\n",
    "holiday_dates_set = set(all_holiday_dates)\n",
    "\n",
    "# --- Filter to top 180 routes ---\n",
    "top_routes = ranked_routes.head(180)['route_no'].astype(str)\n",
    "ridership_df = ridership_df[ridership_df['route_no'].astype(str).isin(top_routes)]\n",
    "\n",
    "# Flag holidays in ridership data\n",
    "ridership_df['is_holiday'] = ridership_df['date'].apply(lambda x: 1 if x in holiday_dates_set else 0)\n",
    "\n",
    "# --- Aggregate ridership per route per day ---\n",
    "agg_df = ridership_df.groupby(['route_no', 'date']).agg({\n",
    "    'ridership_total': 'sum',\n",
    "    'hour': 'max',\n",
    "    'depot': 'first',\n",
    "    'is_holiday': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# --- Calculate hours left before schedule ends ---\n",
    "END_HOUR = 24\n",
    "agg_df['hours_left'] = END_HOUR - agg_df['hour']\n",
    "agg_df['hours_left'] = agg_df['hours_left'].clip(lower=0)\n",
    "# Calculate hours elapsed in the day (max hour in data)\n",
    "agg_df['hours_elapsed'] = agg_df['hour']\n",
    "\n",
    "# Calculate average ridership per hour so far\n",
    "agg_df['avg_ridership_per_hour'] = agg_df['ridership_total'] / agg_df['hours_elapsed'].replace(0, 1)  # avoid division by zero\n",
    "\n",
    "# Estimate total ridership for the full day by multiplying avg ridership per hour by 24 hours\n",
    "agg_df['ridership_total'] = agg_df['avg_ridership_per_hour'] * 24\n",
    "\n",
    "# Alternatively, estimate ridership remaining by multiplying avg ridership per hour by hours left\n",
    "agg_df['estimated_ridership_remaining'] = agg_df['avg_ridership_per_hour'] * agg_df['hours_left']\n",
    "\n",
    "\n",
    "# --- Add temporal features ---\n",
    "agg_df['day_of_week'] = agg_df['date'].dt.dayofweek\n",
    "agg_df['month'] = agg_df['date'].dt.month\n",
    "\n",
    "# --- Map categorical columns using your fixed mappings ---\n",
    "agg_df['depot_enc'] = agg_df['depot'].map(depot_mapping).fillna(-1).astype(int)\n",
    "agg_df['route_no_enc'] = agg_df['route_no'].map(route_no_mapping).fillna(-1).astype(int)\n",
    "\n",
    "\n",
    "# Save to CSV for further use or inspection\n",
    "agg_df.to_csv('/Users/fahmi.taib/Desktop/Deployment Code Test/agg_ridership_with_estimates.csv', index=False)\n",
    "\n",
    "print(\"Aggregated ridership with estimates saved successfully.\")\n",
    "\n",
    "# # --- Prepare features and target ---\n",
    "# features = ['route_no_enc', 'day_of_week', 'month', 'depot_enc', 'is_holiday', 'hours_left']\n",
    "# X = agg_df[features]\n",
    "# y = agg_df['ridership_total']\n",
    "\n",
    "# # --- Train/test split ---\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # --- Scale features ---\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # --- Define XGBoost model ---\n",
    "# xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# # --- Hyperparameter distributions ---\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(100, 500),\n",
    "#     'max_depth': randint(3, 10),\n",
    "#     'learning_rate': uniform(0.01, 0.3),\n",
    "#     'subsample': uniform(0.6, 0.4),\n",
    "#     'colsample_bytree': uniform(0.6, 0.4),\n",
    "#     'gamma': uniform(0, 5),\n",
    "#     'reg_alpha': uniform(0, 1),\n",
    "#     'reg_lambda': uniform(0, 1)\n",
    "# }\n",
    "\n",
    "# # --- Randomized Search ---\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=xgb,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=50,\n",
    "#     scoring='neg_mean_absolute_error',\n",
    "#     cv=3,\n",
    "#     verbose=1,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# print(\"Starting hyperparameter tuning...\")\n",
    "# random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "\n",
    "# # --- Predict and evaluate ---\n",
    "# best_model = random_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# rmse = np.sqrt(mse)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(\"\\nXGBoost Model Evaluation:\")\n",
    "# print(f\"MAE: {mae:.2f}\")\n",
    "# print(f\"MSE: {mse:.2f}\")\n",
    "# print(f\"RMSE: {rmse:.2f}\")\n",
    "# print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# # --- Save model and scaler ---\n",
    "# joblib.dump(best_model, '/Users/fahmi.taib/Desktop/Deployment Code Test/xgb_ridership_model.pkl')\n",
    "# joblib.dump(scaler, '/Users/fahmi.taib/Desktop/Deployment Code Test/xgb_feature_scaler.pkl')\n",
    "\n",
    "# print(\"Model and scaler saved successfully.\")\n",
    "\n",
    "# --- Calculate average ridership per hour and estimated total ridership ---\n",
    "agg_df['hours_elapsed'] = agg_df['hour'].replace(0, 1)  # avoid division by zero\n",
    "agg_df['avg_ridership_per_hour'] = agg_df['ridership_total'] / agg_df['hours_elapsed']\n",
    "agg_df['ridership_total'] = agg_df['avg_ridership_per_hour'] * 24  # full day estimate\n",
    "\n",
    "# --- Prepare features and target ---\n",
    "features = ['route_no_enc', 'day_of_week', 'month', 'depot_enc', 'is_holiday', 'hours_left']\n",
    "X = agg_df[features]\n",
    "y = agg_df['ridership_total']  \n",
    "\n",
    "# --- Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Scale features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Define XGBoost model ---\n",
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# --- Hyperparameter distributions ---\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1)\n",
    "}\n",
    "\n",
    "# --- Randomized Search ---\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "\n",
    "# --- Predict and evaluate ---\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nXGBoost Model Evaluation:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# --- Save model and scaler ---\n",
    "joblib.dump(best_model, '/Users/fahmi.taib/Desktop/Deployment Code Test/xgb_ridership_model.pkl')\n",
    "joblib.dump(scaler, '/Users/fahmi.taib/Desktop/Deployment Code Test/xgb_feature_scaler.pkl')\n",
    "\n",
    "print(\"Model and scaler saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ebbbec3-7a44-4731-a53a-ada30117eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "def unique_preserve_order(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in seq:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "# Clean your categories before using them\n",
    "route_no_categories = unique_preserve_order(route_no_categories)\n",
    "depot_categories = unique_preserve_order(depot_categories)\n",
    "\n",
    "# Now create CategoricalDtype objects\n",
    "depot_cat_type = CategoricalDtype(categories=depot_categories, ordered=True)\n",
    "route_no_cat_type = CategoricalDtype(categories=route_no_categories, ordered=True)\n",
    "\n",
    "# Proceed with encoding\n",
    "agg_df['depot_enc'] = agg_df['depot'].astype(depot_cat_type).cat.codes\n",
    "agg_df['route_no_enc'] = agg_df['route_no'].astype(route_no_cat_type).cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d1a890-8562-44b5-91f7-8d7d725e4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 175\n",
      "Cleaned length: 175\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original length: {len(route_no_categories)}\")\n",
    "route_no_categories = unique_preserve_order(route_no_categories)\n",
    "print(f\"Cleaned length: {len(route_no_categories)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
